{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c10a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fece7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: yellow_tripdata_2019-02.parquet\n",
      "Downloaded: yellow_tripdata_2019-03.parquet\n",
      "All files downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# URL of the NYC TLC Trip Record Data page\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the div with id \"faq2019\" containing the links\n",
    "faq2019_div = soup.find(\"div\", id=\"faq2019\")\n",
    "\n",
    "# Initialize variables to store URLs\n",
    "parquet_urls = []\n",
    "\n",
    "# Loop through all links in the faq2019_div\n",
    "for link in faq2019_div.find_all(\"a\", href=True):\n",
    "    href = link[\"href\"]\n",
    "    if \"parquet\" in href and \"yellow_tripdata\" in href and (\"2019-02\" in href or \"2019-03\" in href):\n",
    "        parquet_urls.append(href)\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Create a directory to save the files\n",
    "os.makedirs(\"parquet_files\", exist_ok=True)\n",
    "\n",
    "# Download each Parquet file\n",
    "for url in parquet_urls:\n",
    "    filename = os.path.basename(url)\n",
    "    save_path = os.path.join(\"parquet_files\", filename)\n",
    "    download_file(url, save_path)\n",
    "    print(f\"Downloaded: {filename}\")\n",
    "\n",
    "print(\"All files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219d1827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: yellow_tripdata_2019-02.parquet to parquet_files\\tripdata\n",
      "Downloaded: green_tripdata_2019-02.parquet to parquet_files\\tripdata\n",
      "Downloaded: fhv_tripdata_2019-02.parquet to parquet_files\\tripdata\n",
      "Downloaded: fhvhv_tripdata_2019-02.parquet to parquet_files\\tripdata\n",
      "Downloaded: yellow_tripdata_2019-03.parquet to parquet_files\\tripdata\n",
      "Downloaded: green_tripdata_2019-03.parquet to parquet_files\\tripdata\n",
      "Downloaded: fhv_tripdata_2019-03.parquet to parquet_files\\tripdata\n",
      "Downloaded: fhvhv_tripdata_2019-03.parquet to parquet_files\\tripdata\n",
      "All files downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the NYC TLC Trip Record Data page\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the div with id \"faq2019\" containing the links\n",
    "faq2019_div = soup.find(\"div\", id=\"faq2019\")\n",
    "\n",
    "# Initialize a dictionary to store URLs by month\n",
    "parquet_urls_by_month = {}\n",
    "\n",
    "# Loop through all links in the faq2019_div\n",
    "for link in faq2019_div.find_all(\"a\", href=True):\n",
    "    href = link[\"href\"]\n",
    "    # Check if the link is a Parquet file and belongs to a specific month\n",
    "    if \"parquet\" in href and (\"2019-02\" in href or \"2019-03\" in href):\n",
    "        month = href.split('_')[1]  # Extract month from the URL (assuming URL format is consistent)\n",
    "        if month not in parquet_urls_by_month:\n",
    "            parquet_urls_by_month[month] = []\n",
    "        parquet_urls_by_month[month].append(href)\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Create a directory to save the files\n",
    "os.makedirs(\"parquet_files\", exist_ok=True)\n",
    "\n",
    "# Download each Parquet file for each month\n",
    "for month, urls in parquet_urls_by_month.items():\n",
    "    month_directory = os.path.join(\"parquet_files\", month)\n",
    "    os.makedirs(month_directory, exist_ok=True)  # Create month directory\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        save_path = os.path.join(month_directory, filename)\n",
    "        download_file(url, save_path)\n",
    "        print(f\"Downloaded: {filename} to {month_directory}\")\n",
    "\n",
    "print(\"All files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad07f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (2.0.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pymongo\n",
      "  Obtaining dependency information for pymongo from https://files.pythonhosted.org/packages/51/28/577224211f43e2079126bfec53080efba46e59218f47808098f125139558/pymongo-4.8.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pymongo-4.8.0-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/87/a1/8c5287991ddb8d3e4662f71356d9656d91ab3a36618c3dd11b280df0d255/dnspython-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijaya.reddy\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pymongo-4.8.0-cp311-cp311-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/631.0 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 41.0/631.0 kB 960.0 kB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 174.1/631.0 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 419.8/631.0 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  624.6/631.0 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 631.0/631.0 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------  307.2/307.7 kB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 307.7/307.7 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.6.1 pymongo-4.8.0\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pymongo pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parquet file: parquet_files/tripdata\\fhvhv_tripdata_2019-02.parquet\n",
      "Data preview for collection 'fhvhv_tripdata_2019-02':\n",
      "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
      "0            HV0003               B02867               B02867   \n",
      "1            HV0003               B02879               B02879   \n",
      "2            HV0005               B02510                 None   \n",
      "3            HV0005               B02510                 None   \n",
      "4            HV0005               B02510                 None   \n",
      "\n",
      "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
      "0 2019-02-01 00:01:26 2019-02-01 00:02:55 2019-02-01 00:05:18   \n",
      "1 2019-02-01 00:26:08 2019-02-01 00:41:29 2019-02-01 00:41:29   \n",
      "2 2019-02-01 00:48:58                 NaT 2019-02-01 00:51:34   \n",
      "3 2019-02-01 00:02:15                 NaT 2019-02-01 00:03:51   \n",
      "4 2019-02-01 00:06:17                 NaT 2019-02-01 00:09:44   \n",
      "\n",
      "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  sales_tax  \\\n",
      "0 2019-02-01 00:14:57           245           251        2.45  ...       0.83   \n",
      "1 2019-02-01 00:49:39           216           197        1.71  ...       0.70   \n",
      "2 2019-02-01 01:28:29           261           234        5.01  ...       3.99   \n",
      "3 2019-02-01 00:07:16            87            87        0.34  ...       0.64   \n",
      "4 2019-02-01 00:39:56            87           198        6.84  ...       2.16   \n",
      "\n",
      "   congestion_surcharge  airport_fee  tips  driver_pay  shared_request_flag  \\\n",
      "0                   0.0         None   0.0        7.48                    Y   \n",
      "1                   0.0         None   2.0        7.93                    N   \n",
      "2                   0.0         None   0.0       35.97                    N   \n",
      "3                   0.0         None   3.0        5.39                    N   \n",
      "4                   0.0         None   4.0       17.07                    N   \n",
      "\n",
      "  shared_match_flag  access_a_ride_flag  wav_request_flag wav_match_flag  \n",
      "0                 N                   N                 N           None  \n",
      "1                 N                   N                 N           None  \n",
      "2                 Y                   N                 N           None  \n",
      "3                 Y                   N                 N           None  \n",
      "4                 Y                   N                 N           None  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow.compute as pc\n",
    "# from pymongo import MongoClient, errors\n",
    "\n",
    "# # MongoDB connection settings\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['apnabase']\n",
    "\n",
    "# # Directory where Parquet files are saved\n",
    "# parquet_directory = \"parquet_files/tripdata\"\n",
    "\n",
    "# # Iterate through each file in the parquet directory\n",
    "# for root, dirs, files in os.walk(parquet_directory):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".parquet\"):  # Target specific file\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             collection_name = os.path.splitext(file)[0]  # Use file name as collection name\n",
    "\n",
    "#             # Print Parquet file information\n",
    "#             print(f\"Processing Parquet file: {file_path}\")\n",
    "\n",
    "#             # Read Parquet file into a table using pyarrow and filter invalid datetime values\n",
    "#             try:\n",
    "#                 table = pq.read_table(file_path)\n",
    "#                 df = table.filter(\n",
    "#                     pc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n",
    "#                 ).to_pandas()\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to read and process Parquet file {file_path}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Display data (first few rows)\n",
    "#             print(f\"Data preview for collection '{collection_name}':\")\n",
    "#             print(df.head())  # Display the first few rows of the dataframe\n",
    "#             print(\"\\n\")\n",
    "\n",
    "#             # Convert datetime columns to UTC before inserting into MongoDB\n",
    "#             for col in df.select_dtypes(include=['datetime64[ns]']):\n",
    "#                 try:\n",
    "#                     # Localize timestamps (assuming your local timezone)\n",
    "#                     # Replace 'Asia/Kolkata' with your actual local timezone if needed\n",
    "#                     df[col] = df[col].dt.tz_localize('Asia/Kolkata', ambiguous='NaT', nonexistent='shift_forward')\n",
    "#                     # Convert to UTC\n",
    "#                     df[col] = df[col].dt.tz_convert('UTC')\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error localizing timezone for column {col}: {e}\")\n",
    "\n",
    "#             # Convert dataframe to records (list of dictionaries)\n",
    "#             records = df.to_dict(orient='records')\n",
    "\n",
    "#             # Batch insertion into MongoDB collection\n",
    "#             chunk_size = 1000  # Adjust as needed\n",
    "#             try:\n",
    "#                 collection = db[collection_name]\n",
    "#                 for i in range(0, len(records), chunk_size):\n",
    "#                     collection.insert_many(records[i:i + chunk_size])\n",
    "#                 print(f\"Inserted {len(records)} documents into collection '{collection_name}' in MongoDB.\")\n",
    "#             except errors.BulkWriteError as bwe:\n",
    "#                 print(f\"Bulk write error while inserting documents into collection '{collection_name}': {bwe.details}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"An error occurred while inserting documents into collection '{collection_name}': {e}\")\n",
    "#             print(\"\\n\")\n",
    "\n",
    "# # Close MongoDB connection\n",
    "# client.close()\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "# MongoDB connection settings\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['apnabase']\n",
    "\n",
    "# Directory where Parquet files are saved\n",
    "parquet_directory = \"parquet_files/tripdata\"\n",
    "\n",
    "# Iterate through each file in the parquet directory\n",
    "for root, dirs, files in os.walk(parquet_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):  # Target specific file\n",
    "            file_path = os.path.join(root, file)\n",
    "            collection_name = os.path.splitext(file)[0]  # Use file name as collection name\n",
    "\n",
    "            # Print Parquet file information\n",
    "            print(f\"Processing Parquet file: {file_path}\")\n",
    "\n",
    "            # Read Parquet file into a table using pyarrow and filter invalid datetime values if applicable\n",
    "            try:\n",
    "                table = pq.read_table(file_path)\n",
    "                if \"dropOff_datetime\" in table.schema.names:\n",
    "                    df = table.filter(\n",
    "                        pc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n",
    "                    ).to_pandas()\n",
    "                else:\n",
    "                    df = table.to_pandas()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read and process Parquet file {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Display data (first few rows)\n",
    "            print(f\"Data preview for collection '{collection_name}':\")\n",
    "            print(df.head())  # Display the first few rows of the dataframe\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # Convert datetime columns to UTC before inserting into MongoDB\n",
    "            for col in df.select_dtypes(include=['datetime64[ns]']):\n",
    "                try:\n",
    "                    # Localize timestamps (assuming your local timezone)\n",
    "                    # Replace 'Asia/Kolkata' with your actual local timezone if needed\n",
    "                    df[col] = df[col].dt.tz_localize('Asia/Kolkata', ambiguous='NaT', nonexistent='shift_forward')\n",
    "                    # Convert to UTC\n",
    "                    df[col] = df[col].dt.tz_convert('UTC')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error localizing timezone for column {col}: {e}\")\n",
    "\n",
    "            # Convert dataframe to records (list of dictionaries)\n",
    "            records = df.to_dict(orient='records')\n",
    "\n",
    "            # Batch insertion into MongoDB collection\n",
    "            chunk_size = 1000  # Adjust as needed\n",
    "            try:\n",
    "                collection = db[collection_name]\n",
    "                for i in range(0, len(records), chunk_size):\n",
    "                    collection.insert_many(records[i:i + chunk_size])\n",
    "                print(f\"Inserted {len(records)} documents into collection '{collection_name}' in MongoDB.\")\n",
    "            except errors.BulkWriteError as bwe:\n",
    "                print(f\"Bulk write error while inserting documents into collection '{collection_name}': {bwe.details}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while inserting documents into collection '{collection_name}': {e}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46e550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
